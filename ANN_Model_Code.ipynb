{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Harsh24032000/Fault-Detection-in-Power-Microgrid/blob/master/ANN_Model_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\programdata\\anaconda4\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: absl-py in c:\\programdata\\anaconda4\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda4\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda4\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\programdata\\anaconda4\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda4\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\programdata\\anaconda4\\lib\\site-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\programdata\\anaconda4\\lib\\site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda4\\lib\\site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda4\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda4\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda4\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda4\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda4\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda4\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda4\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda4\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda4\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scikit (from versions: none)\n",
      "ERROR: No matching distribution found for scikit\n"
     ]
    }
   ],
   "source": [
    "! pip install keras\n",
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  rather than 'sklearn' for pip commands.\n",
      "  \n",
      "  Here is how to fix this error in the main use cases:\n",
      "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  - if the 'sklearn' package is used by one of your dependencies,\n",
      "    it would be great if you take some time to track which package uses\n",
      "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  - as a last resort, set the environment variable\n",
      "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \n",
      "  More information is available at\n",
      "  https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from scikeras) (3.8.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in c:\\programdata\\anaconda4\\lib\\site-packages (from scikeras) (1.5.1)\n",
      "Requirement already satisfied: absl-py in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.2.0->scikeras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.14.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda4\\lib\\site-packages (from keras>=3.2.0->scikeras) (24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from optree->keras>=3.2.0->scikeras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda4\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda4\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.0)\n",
      "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.13.0\n"
     ]
    }
   ],
   "source": [
    "! pip install sklearn\n",
    "! pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vh_2PFdoc83n",
    "outputId": "3c535a50-5853-468d-b9f9-5a1d148a13ea"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4mPCB1wwZQC"
   },
   "outputs": [],
   "source": [
    "seed=7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "dOR-kUC5dPB7",
    "outputId": "eb51bf24-927c-49ee-851f-91541dde974c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Current 1</th>\n",
       "      <th>P_L 1</th>\n",
       "      <th>Current 2</th>\n",
       "      <th>P_L 2</th>\n",
       "      <th>Current 3</th>\n",
       "      <th>P_L 3</th>\n",
       "      <th>Current Ideal</th>\n",
       "      <th>P_L Ideal</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.858</td>\n",
       "      <td>492.0</td>\n",
       "      <td>5.249</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>1.9690</td>\n",
       "      <td>1640</td>\n",
       "      <td>9.465000e-05</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.429</td>\n",
       "      <td>246.0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>546.7</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>820</td>\n",
       "      <td>8.514000e-05</td>\n",
       "      <td>984</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.429</td>\n",
       "      <td>246.0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>546.7</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>820</td>\n",
       "      <td>-9.609000e-11</td>\n",
       "      <td>984</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.429</td>\n",
       "      <td>246.0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>546.7</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>820</td>\n",
       "      <td>-9.609000e-11</td>\n",
       "      <td>984</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.429</td>\n",
       "      <td>246.0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>546.7</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>820</td>\n",
       "      <td>-9.609000e-11</td>\n",
       "      <td>984</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.382</td>\n",
       "      <td>410.0</td>\n",
       "      <td>4.374</td>\n",
       "      <td>911.1</td>\n",
       "      <td>1.6400</td>\n",
       "      <td>1670</td>\n",
       "      <td>5.178000e-06</td>\n",
       "      <td>1640</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.760</td>\n",
       "      <td>819.9</td>\n",
       "      <td>8.748</td>\n",
       "      <td>1822.0</td>\n",
       "      <td>3.2800</td>\n",
       "      <td>2733</td>\n",
       "      <td>-9.125000e-05</td>\n",
       "      <td>3280</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.570</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>15.750</td>\n",
       "      <td>3280.0</td>\n",
       "      <td>5.9050</td>\n",
       "      <td>4919</td>\n",
       "      <td>-1.579000e-04</td>\n",
       "      <td>5903</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29.530</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>17.500</td>\n",
       "      <td>3644.0</td>\n",
       "      <td>6.5610</td>\n",
       "      <td>5466</td>\n",
       "      <td>5.177000e-06</td>\n",
       "      <td>6559</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.760</td>\n",
       "      <td>819.9</td>\n",
       "      <td>8.749</td>\n",
       "      <td>1822.0</td>\n",
       "      <td>3.2810</td>\n",
       "      <td>2733</td>\n",
       "      <td>3.462000e-04</td>\n",
       "      <td>3280</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.858</td>\n",
       "      <td>492.0</td>\n",
       "      <td>5.249</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>1.9690</td>\n",
       "      <td>1640</td>\n",
       "      <td>1.385000e-04</td>\n",
       "      <td>1968</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.120</td>\n",
       "      <td>983.9</td>\n",
       "      <td>14.900</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>8.3410</td>\n",
       "      <td>3280</td>\n",
       "      <td>4.405000e+00</td>\n",
       "      <td>3936</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25.170</td>\n",
       "      <td>656.0</td>\n",
       "      <td>20.360</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>15.9800</td>\n",
       "      <td>2187</td>\n",
       "      <td>1.336000e+01</td>\n",
       "      <td>2624</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>27.720</td>\n",
       "      <td>410.0</td>\n",
       "      <td>24.710</td>\n",
       "      <td>911.1</td>\n",
       "      <td>21.9800</td>\n",
       "      <td>1367</td>\n",
       "      <td>2.034000e+01</td>\n",
       "      <td>1640</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>28.710</td>\n",
       "      <td>246.0</td>\n",
       "      <td>26.900</td>\n",
       "      <td>546.7</td>\n",
       "      <td>25.2600</td>\n",
       "      <td>820</td>\n",
       "      <td>2.428000e+01</td>\n",
       "      <td>984</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26.060</td>\n",
       "      <td>328.0</td>\n",
       "      <td>23.650</td>\n",
       "      <td>728.9</td>\n",
       "      <td>21.4600</td>\n",
       "      <td>1093</td>\n",
       "      <td>2.015000e+01</td>\n",
       "      <td>1312</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.790</td>\n",
       "      <td>819.9</td>\n",
       "      <td>12.780</td>\n",
       "      <td>1822.0</td>\n",
       "      <td>7.3100</td>\n",
       "      <td>2733</td>\n",
       "      <td>4.030000e+00</td>\n",
       "      <td>3280</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.311</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>-3.515</td>\n",
       "      <td>3280.0</td>\n",
       "      <td>-13.3600</td>\n",
       "      <td>4919</td>\n",
       "      <td>-1.926000e+01</td>\n",
       "      <td>5903</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33.960</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>20.120</td>\n",
       "      <td>4191.0</td>\n",
       "      <td>7.5440</td>\n",
       "      <td>6286</td>\n",
       "      <td>-2.698000e-04</td>\n",
       "      <td>7543</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32.480</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>19.250</td>\n",
       "      <td>4008.0</td>\n",
       "      <td>7.2170</td>\n",
       "      <td>6013</td>\n",
       "      <td>-6.212000e-05</td>\n",
       "      <td>7215</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32.480</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>19.250</td>\n",
       "      <td>4008.0</td>\n",
       "      <td>7.2170</td>\n",
       "      <td>6013</td>\n",
       "      <td>-7.047000e-10</td>\n",
       "      <td>7215</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33.960</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>20.120</td>\n",
       "      <td>4191.0</td>\n",
       "      <td>7.5450</td>\n",
       "      <td>6286</td>\n",
       "      <td>-2.718000e-05</td>\n",
       "      <td>7543</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>26.570</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>15.750</td>\n",
       "      <td>3280.0</td>\n",
       "      <td>5.9050</td>\n",
       "      <td>4919</td>\n",
       "      <td>1.359000e-04</td>\n",
       "      <td>5903</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Current 1   P_L 1  Current 2   P_L 2  Current 3  P_L 3  Current Ideal  \\\n",
       "0       8.858   492.0      5.249  1093.0     1.9690   1640   9.465000e-05   \n",
       "1       4.429   246.0      2.625   546.7     0.9844    820   8.514000e-05   \n",
       "2       4.429   246.0      2.625   546.7     0.9844    820  -9.609000e-11   \n",
       "3       4.429   246.0      2.625   546.7     0.9844    820  -9.609000e-11   \n",
       "4       4.429   246.0      2.625   546.7     0.9844    820  -9.609000e-11   \n",
       "5       7.382   410.0      4.374   911.1     1.6400   1670   5.178000e-06   \n",
       "6      14.760   819.9      8.748  1822.0     3.2800   2733  -9.125000e-05   \n",
       "7      26.570  1476.0     15.750  3280.0     5.9050   4919  -1.579000e-04   \n",
       "8      29.530  1640.0     17.500  3644.0     6.5610   5466   5.177000e-06   \n",
       "9      14.760   819.9      8.749  1822.0     3.2810   2733   3.462000e-04   \n",
       "10      8.858   492.0      5.249  1093.0     1.9690   1640   1.385000e-04   \n",
       "11     22.120   983.9     14.900  2186.0     8.3410   3280   4.405000e+00   \n",
       "12     25.170   656.0     20.360  1458.0    15.9800   2187   1.336000e+01   \n",
       "13     27.720   410.0     24.710   911.1    21.9800   1367   2.034000e+01   \n",
       "14     28.710   246.0     26.900   546.7    25.2600    820   2.428000e+01   \n",
       "15     26.060   328.0     23.650   728.9    21.4600   1093   2.015000e+01   \n",
       "16     18.790   819.9     12.780  1822.0     7.3100   2733   4.030000e+00   \n",
       "17      7.311  1476.0     -3.515  3280.0   -13.3600   4919  -1.926000e+01   \n",
       "18     33.960  1886.0     20.120  4191.0     7.5440   6286  -2.698000e-04   \n",
       "19     32.480  1804.0     19.250  4008.0     7.2170   6013  -6.212000e-05   \n",
       "20     32.480  1804.0     19.250  4008.0     7.2170   6013  -7.047000e-10   \n",
       "21     33.960  1886.0     20.120  4191.0     7.5450   6286  -2.718000e-05   \n",
       "22     26.570  1476.0     15.750  3280.0     5.9050   4919   1.359000e-04   \n",
       "\n",
       "    P_L Ideal  Time  \n",
       "0        1968     1  \n",
       "1         984     2  \n",
       "2         984     3  \n",
       "3         984     4  \n",
       "4         984     5  \n",
       "5        1640     6  \n",
       "6        3280     7  \n",
       "7        5903     8  \n",
       "8        6559     9  \n",
       "9        3280    10  \n",
       "10       1968    11  \n",
       "11       3936    12  \n",
       "12       2624    13  \n",
       "13       1640    14  \n",
       "14        984    15  \n",
       "15       1312    16  \n",
       "16       3280    17  \n",
       "17       5903    18  \n",
       "18       7543    19  \n",
       "19       7215    20  \n",
       "20       7215    21  \n",
       "21       7543    22  \n",
       "22       5903    23  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel('Data.xlsx')\n",
    "df1=df.values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOOARm0VdcBp"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current</th>\n",
       "      <th>load</th>\n",
       "      <th>result</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [current, load, result, Time]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "po=pd.DataFrame(columns=['current','load','result','Time'])\n",
    "po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12924\\1002947168.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  po = pd.concat([po, row_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "po = pd.DataFrame(columns=['current', 'load', 'result', 'Time'])\n",
    "\n",
    "for i in range(23):\n",
    "    # Create a dictionary for the current row\n",
    "    row_data = {\n",
    "        'current': df.at[i, \"Current 1\"],\n",
    "        'load': df.at[i, \"P_L 1\"],\n",
    "        'result': 1,\n",
    "        'Time': df.at[i, \"Time\"]\n",
    "    }\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    row_df = pd.DataFrame([row_data])\n",
    "    \n",
    "    # Concatenate the row DataFrame to `po`\n",
    "    po = pd.concat([po, row_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "oGNxLOtXdlhY",
    "outputId": "13485257-89c0-4378-e9c9-c63977b1cff6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current</th>\n",
       "      <th>load</th>\n",
       "      <th>result</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.858</td>\n",
       "      <td>492.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.429</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.429</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.429</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.429</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.382</td>\n",
       "      <td>410.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.760</td>\n",
       "      <td>819.9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.570</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29.530</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.760</td>\n",
       "      <td>819.9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.858</td>\n",
       "      <td>492.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.120</td>\n",
       "      <td>983.9</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25.170</td>\n",
       "      <td>656.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>27.720</td>\n",
       "      <td>410.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>28.710</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26.060</td>\n",
       "      <td>328.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.790</td>\n",
       "      <td>819.9</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.311</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33.960</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32.480</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32.480</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33.960</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>26.570</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    current    load  result  Time\n",
       "0     8.858   492.0       1     1\n",
       "1     4.429   246.0       1     2\n",
       "2     4.429   246.0       1     3\n",
       "3     4.429   246.0       1     4\n",
       "4     4.429   246.0       1     5\n",
       "5     7.382   410.0       1     6\n",
       "6    14.760   819.9       1     7\n",
       "7    26.570  1476.0       1     8\n",
       "8    29.530  1640.0       1     9\n",
       "9    14.760   819.9       1    10\n",
       "10    8.858   492.0       1    11\n",
       "11   22.120   983.9       1    12\n",
       "12   25.170   656.0       1    13\n",
       "13   27.720   410.0       1    14\n",
       "14   28.710   246.0       1    15\n",
       "15   26.060   328.0       1    16\n",
       "16   18.790   819.9       1    17\n",
       "17    7.311  1476.0       1    18\n",
       "18   33.960  1886.0       1    19\n",
       "19   32.480  1804.0       1    20\n",
       "20   32.480  1804.0       1    21\n",
       "21   33.960  1886.0       1    22\n",
       "22   26.570  1476.0       1    23"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UzQk6EOdq3Q"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12924\\814739756.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m23\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'current'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Current 3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'load'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"P_L 3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Time'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\anaconda4\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# for i in range(23):\n",
    "#     po=po.append({'current':df.at[i,\"Current 3\"],'load':df.at[i,\"P_L 3\"],'result':3,'Time':df.at[i,\"Time\"]},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for i in range(23):\n",
    "    row_data = {\n",
    "        'current': df.at[i, \"Current 1\"],\n",
    "        'load': df.at[i, \"P_L 1\"],\n",
    "        'result': 1,\n",
    "        'Time': df.at[i, \"Time\"]\n",
    "    }\n",
    "    rows.append(row_data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "po = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DjyG5rm2dz2m",
    "outputId": "4ba00cec-1721-4da0-e0f4-09f6e2a0878a"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12924\\2037805095.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m23\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'current'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Current Ideal\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'load'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"P_L Ideal\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Time'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\anaconda4\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "for i in range(23):\n",
    "    # po=po.append({'current':df.at[i,\"Current Ideal\"],'load':df.at[i,\"P_L Ideal\"],'result':0,'Time':df.at[i,\"Time\"]},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12924\\2078162155.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  po = pd.concat([po, row_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "po = pd.DataFrame(columns=['current', 'load', 'result', 'Time'])\n",
    "\n",
    "for i in range(23):\n",
    "    # Create a dictionary for the current row\n",
    "    row_data = {\n",
    "        'current': df.at[i, \"Current Ideal\"],\n",
    "        'load': df.at[i, \"P_L Ideal\"],\n",
    "        'result': 0,\n",
    "        'Time': df.at[i, \"Time\"]\n",
    "    }\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    row_df = pd.DataFrame([row_data])\n",
    "    \n",
    "    # Concatenate the row DataFrame to `po`\n",
    "    po = pd.concat([po, row_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "r-5cNtPCd2YG",
    "outputId": "8ad1e5ba-02c1-4bb2-b69e-93516aaabf3a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current</th>\n",
       "      <th>load</th>\n",
       "      <th>result</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.465000e-05</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.514000e-05</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9.609000e-11</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.609000e-11</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.609000e-11</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.178000e-06</td>\n",
       "      <td>1640</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-9.125000e-05</td>\n",
       "      <td>3280</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.579000e-04</td>\n",
       "      <td>5903</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.177000e-06</td>\n",
       "      <td>6559</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.462000e-04</td>\n",
       "      <td>3280</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.385000e-04</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.405000e+00</td>\n",
       "      <td>3936</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.336000e+01</td>\n",
       "      <td>2624</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.034000e+01</td>\n",
       "      <td>1640</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.428000e+01</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.015000e+01</td>\n",
       "      <td>1312</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.030000e+00</td>\n",
       "      <td>3280</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.926000e+01</td>\n",
       "      <td>5903</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-2.698000e-04</td>\n",
       "      <td>7543</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-6.212000e-05</td>\n",
       "      <td>7215</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-7.047000e-10</td>\n",
       "      <td>7215</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-2.718000e-05</td>\n",
       "      <td>7543</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.359000e-04</td>\n",
       "      <td>5903</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         current  load result Time\n",
       "0   9.465000e-05  1968      0    1\n",
       "1   8.514000e-05   984      0    2\n",
       "2  -9.609000e-11   984      0    3\n",
       "3  -9.609000e-11   984      0    4\n",
       "4  -9.609000e-11   984      0    5\n",
       "5   5.178000e-06  1640      0    6\n",
       "6  -9.125000e-05  3280      0    7\n",
       "7  -1.579000e-04  5903      0    8\n",
       "8   5.177000e-06  6559      0    9\n",
       "9   3.462000e-04  3280      0   10\n",
       "10  1.385000e-04  1968      0   11\n",
       "11  4.405000e+00  3936      0   12\n",
       "12  1.336000e+01  2624      0   13\n",
       "13  2.034000e+01  1640      0   14\n",
       "14  2.428000e+01   984      0   15\n",
       "15  2.015000e+01  1312      0   16\n",
       "16  4.030000e+00  3280      0   17\n",
       "17 -1.926000e+01  5903      0   18\n",
       "18 -2.698000e-04  7543      0   19\n",
       "19 -6.212000e-05  7215      0   20\n",
       "20 -7.047000e-10  7215      0   21\n",
       "21 -2.718000e-05  7543      0   22\n",
       "22  1.359000e-04  5903      0   23"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t00qelT1d6lX"
   },
   "outputs": [],
   "source": [
    "po = pd.concat([po,pd.get_dummies(po['Time'], prefix='Time',dummy_na=True)],axis=1).drop(['Time'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtP2JRm5eBDI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current</th>\n",
       "      <th>load</th>\n",
       "      <th>result</th>\n",
       "      <th>Time_1.0</th>\n",
       "      <th>Time_2.0</th>\n",
       "      <th>Time_3.0</th>\n",
       "      <th>Time_4.0</th>\n",
       "      <th>Time_5.0</th>\n",
       "      <th>Time_6.0</th>\n",
       "      <th>Time_7.0</th>\n",
       "      <th>...</th>\n",
       "      <th>Time_15.0</th>\n",
       "      <th>Time_16.0</th>\n",
       "      <th>Time_17.0</th>\n",
       "      <th>Time_18.0</th>\n",
       "      <th>Time_19.0</th>\n",
       "      <th>Time_20.0</th>\n",
       "      <th>Time_21.0</th>\n",
       "      <th>Time_22.0</th>\n",
       "      <th>Time_23.0</th>\n",
       "      <th>Time_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.465000e-05</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.514000e-05</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9.609000e-11</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.609000e-11</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.609000e-11</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.178000e-06</td>\n",
       "      <td>1640</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-9.125000e-05</td>\n",
       "      <td>3280</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.579000e-04</td>\n",
       "      <td>5903</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.177000e-06</td>\n",
       "      <td>6559</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.462000e-04</td>\n",
       "      <td>3280</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.385000e-04</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.405000e+00</td>\n",
       "      <td>3936</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.336000e+01</td>\n",
       "      <td>2624</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.034000e+01</td>\n",
       "      <td>1640</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.428000e+01</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.015000e+01</td>\n",
       "      <td>1312</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.030000e+00</td>\n",
       "      <td>3280</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.926000e+01</td>\n",
       "      <td>5903</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-2.698000e-04</td>\n",
       "      <td>7543</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-6.212000e-05</td>\n",
       "      <td>7215</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-7.047000e-10</td>\n",
       "      <td>7215</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-2.718000e-05</td>\n",
       "      <td>7543</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.359000e-04</td>\n",
       "      <td>5903</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         current  load result  Time_1.0  Time_2.0  Time_3.0  Time_4.0  \\\n",
       "0   9.465000e-05  1968      0      True     False     False     False   \n",
       "1   8.514000e-05   984      0     False      True     False     False   \n",
       "2  -9.609000e-11   984      0     False     False      True     False   \n",
       "3  -9.609000e-11   984      0     False     False     False      True   \n",
       "4  -9.609000e-11   984      0     False     False     False     False   \n",
       "5   5.178000e-06  1640      0     False     False     False     False   \n",
       "6  -9.125000e-05  3280      0     False     False     False     False   \n",
       "7  -1.579000e-04  5903      0     False     False     False     False   \n",
       "8   5.177000e-06  6559      0     False     False     False     False   \n",
       "9   3.462000e-04  3280      0     False     False     False     False   \n",
       "10  1.385000e-04  1968      0     False     False     False     False   \n",
       "11  4.405000e+00  3936      0     False     False     False     False   \n",
       "12  1.336000e+01  2624      0     False     False     False     False   \n",
       "13  2.034000e+01  1640      0     False     False     False     False   \n",
       "14  2.428000e+01   984      0     False     False     False     False   \n",
       "15  2.015000e+01  1312      0     False     False     False     False   \n",
       "16  4.030000e+00  3280      0     False     False     False     False   \n",
       "17 -1.926000e+01  5903      0     False     False     False     False   \n",
       "18 -2.698000e-04  7543      0     False     False     False     False   \n",
       "19 -6.212000e-05  7215      0     False     False     False     False   \n",
       "20 -7.047000e-10  7215      0     False     False     False     False   \n",
       "21 -2.718000e-05  7543      0     False     False     False     False   \n",
       "22  1.359000e-04  5903      0     False     False     False     False   \n",
       "\n",
       "    Time_5.0  Time_6.0  Time_7.0  ...  Time_15.0  Time_16.0  Time_17.0  \\\n",
       "0      False     False     False  ...      False      False      False   \n",
       "1      False     False     False  ...      False      False      False   \n",
       "2      False     False     False  ...      False      False      False   \n",
       "3      False     False     False  ...      False      False      False   \n",
       "4       True     False     False  ...      False      False      False   \n",
       "5      False      True     False  ...      False      False      False   \n",
       "6      False     False      True  ...      False      False      False   \n",
       "7      False     False     False  ...      False      False      False   \n",
       "8      False     False     False  ...      False      False      False   \n",
       "9      False     False     False  ...      False      False      False   \n",
       "10     False     False     False  ...      False      False      False   \n",
       "11     False     False     False  ...      False      False      False   \n",
       "12     False     False     False  ...      False      False      False   \n",
       "13     False     False     False  ...      False      False      False   \n",
       "14     False     False     False  ...       True      False      False   \n",
       "15     False     False     False  ...      False       True      False   \n",
       "16     False     False     False  ...      False      False       True   \n",
       "17     False     False     False  ...      False      False      False   \n",
       "18     False     False     False  ...      False      False      False   \n",
       "19     False     False     False  ...      False      False      False   \n",
       "20     False     False     False  ...      False      False      False   \n",
       "21     False     False     False  ...      False      False      False   \n",
       "22     False     False     False  ...      False      False      False   \n",
       "\n",
       "    Time_18.0  Time_19.0  Time_20.0  Time_21.0  Time_22.0  Time_23.0  Time_nan  \n",
       "0       False      False      False      False      False      False     False  \n",
       "1       False      False      False      False      False      False     False  \n",
       "2       False      False      False      False      False      False     False  \n",
       "3       False      False      False      False      False      False     False  \n",
       "4       False      False      False      False      False      False     False  \n",
       "5       False      False      False      False      False      False     False  \n",
       "6       False      False      False      False      False      False     False  \n",
       "7       False      False      False      False      False      False     False  \n",
       "8       False      False      False      False      False      False     False  \n",
       "9       False      False      False      False      False      False     False  \n",
       "10      False      False      False      False      False      False     False  \n",
       "11      False      False      False      False      False      False     False  \n",
       "12      False      False      False      False      False      False     False  \n",
       "13      False      False      False      False      False      False     False  \n",
       "14      False      False      False      False      False      False     False  \n",
       "15      False      False      False      False      False      False     False  \n",
       "16      False      False      False      False      False      False     False  \n",
       "17       True      False      False      False      False      False     False  \n",
       "18      False       True      False      False      False      False     False  \n",
       "19      False      False       True      False      False      False     False  \n",
       "20      False      False      False       True      False      False     False  \n",
       "21      False      False      False      False       True      False     False  \n",
       "22      False      False      False      False      False       True     False  \n",
       "\n",
       "[23 rows x 27 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JU9SttS7eC1y"
   },
   "outputs": [],
   "source": [
    "poo=po.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YoYMY0heEkG"
   },
   "outputs": [],
   "source": [
    "yy=poo[:,2]\n",
    "po.drop(['result'],axis=\"columns\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "xQnkBOb_eGou",
    "outputId": "788eb09f-38c6-4068-e3a0-bcf76239f1ea"
   },
   "outputs": [],
   "source": [
    "XX=poo[:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LP99lYcEeIhq"
   },
   "outputs": [],
   "source": [
    "def neural_net():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=27, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(4, kernel_initializer='normal',activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZb562n-yJzJ"
   },
   "outputs": [],
   "source": [
    "# from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVL4rZZDyeuy"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(yy)\n",
    "encoded_Y = encoder.transform(yy)\n",
    "# dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "dummy_y = to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9LaHICzygN1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0oRbw7uyh8N"
   },
   "outputs": [],
   "source": [
    "dummy_XX=XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLdxBr17ylks"
   },
   "outputs": [],
   "source": [
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XLkUP8O-ypti"
   },
   "outputs": [],
   "source": [
    "dummy_XX=scaler.fit_transform(dummy_XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQCCA6QkyrvO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33101709, -0.68676898,  0.        ,  4.69041576, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33101816, -1.09187104,  0.        , -0.21320072,  4.69041576,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33102779, -1.09187104,  0.        , -0.21320072, -0.21320072,\n",
       "         4.69041576, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33102779, -1.09187104,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072,  4.69041576, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33102779, -1.09187104,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072,  4.69041576, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33102721, -0.821803  ,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072,  4.69041576, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33103812, -0.14663291,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072,  4.69041576,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33104566,  0.93322755,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "         4.69041576, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33102721,  1.20329558,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  4.69041576, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33098863, -0.14663291,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072,  4.69041576, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33101213, -0.68676898,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072,  4.69041576, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [ 0.1672707 ,  0.12343513,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072,  4.69041576,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [ 1.18027024, -0.41670095,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "         4.69041576, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [ 1.9698556 , -0.821803  ,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  4.69041576, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [ 2.41555277, -1.09187104,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072,  4.69041576, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [ 1.94836259, -0.95683702,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072,  4.69041576, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [ 0.12485028, -0.14663291,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072,  4.69041576,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-2.50974038,  0.93322755,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "         4.69041576, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33105831,  1.60839764,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  4.69041576, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33103482,  1.47336362,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072,  4.69041576, -0.21320072, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33102779,  1.47336362,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072,  4.69041576, -0.21320072,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33103087,  1.60839764,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072,  4.69041576,\n",
       "        -0.21320072,  0.        ],\n",
       "       [-0.33101242,  0.93322755,  0.        , -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "        -0.21320072, -0.21320072, -0.21320072, -0.21320072, -0.21320072,\n",
       "         4.69041576,  0.        ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-snfrWbytzP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.4606 - loss: 0.6967\n",
      "Epoch 2/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5293 - loss: 0.6950 \n",
      "Epoch 3/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5343 - loss: 0.6911 \n",
      "Epoch 4/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6264 - loss: 0.6815 \n",
      "Epoch 5/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6700 - loss: 0.6785 \n",
      "Epoch 6/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6086 - loss: 0.6756 \n",
      "Epoch 7/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6584 - loss: 0.6698 \n",
      "Epoch 8/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6085 - loss: 0.6807 \n",
      "Epoch 9/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5274 - loss: 0.6847 \n",
      "Epoch 10/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5982 - loss: 0.6688 \n",
      "Epoch 11/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5745 - loss: 0.6766 \n",
      "Epoch 12/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5899 - loss: 0.6686 \n",
      "Epoch 13/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6369 - loss: 0.6638 \n",
      "Epoch 14/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6415 - loss: 0.6570 \n",
      "Epoch 15/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6527 - loss: 0.6527 \n",
      "Epoch 16/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6181 - loss: 0.6567 \n",
      "Epoch 17/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6469 - loss: 0.6531 \n",
      "Epoch 18/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5833 - loss: 0.6689 \n",
      "Epoch 19/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7148 - loss: 0.6390 \n",
      "Epoch 20/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6355 - loss: 0.6539 \n",
      "Epoch 21/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7098 - loss: 0.6496 \n",
      "Epoch 22/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6249 - loss: 0.6499 \n",
      "Epoch 23/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6249 - loss: 0.6465 \n",
      "Epoch 24/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6058 - loss: 0.6584 \n",
      "Epoch 25/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7061 - loss: 0.6286 \n",
      "Epoch 26/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7311 - loss: 0.6186 \n",
      "Epoch 27/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6622 - loss: 0.6348 \n",
      "Epoch 28/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6902 - loss: 0.6313 \n",
      "Epoch 29/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6835 - loss: 0.6240 \n",
      "Epoch 30/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6123 - loss: 0.6323 \n",
      "Epoch 31/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6869 - loss: 0.6177 \n",
      "Epoch 32/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6919 - loss: 0.6163 \n",
      "Epoch 33/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6913 - loss: 0.6113 \n",
      "Epoch 34/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7603 - loss: 0.6056 \n",
      "Epoch 35/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7258 - loss: 0.5988 \n",
      "Epoch 36/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7005 - loss: 0.6185 \n",
      "Epoch 37/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7539 - loss: 0.5854 \n",
      "Epoch 38/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7381 - loss: 0.5896  \n",
      "Epoch 39/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7436 - loss: 0.5885 \n",
      "Epoch 40/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7401 - loss: 0.6028 \n",
      "Epoch 41/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7794 - loss: 0.6063 \n",
      "Epoch 42/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7195 - loss: 0.5990 \n",
      "Epoch 43/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7830 - loss: 0.5555 \n",
      "Epoch 44/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7659 - loss: 0.5760 \n",
      "Epoch 45/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7938 - loss: 0.5605 \n",
      "Epoch 46/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7474 - loss: 0.5591 \n",
      "Epoch 47/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7753 - loss: 0.5820 \n",
      "Epoch 48/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8015 - loss: 0.5326 \n",
      "Epoch 49/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7386 - loss: 0.5337 \n",
      "Epoch 50/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7131 - loss: 0.5659 \n",
      "Epoch 51/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7783 - loss: 0.5694 \n",
      "Epoch 52/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7148 - loss: 0.5758 \n",
      "Epoch 53/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7811 - loss: 0.5515 \n",
      "Epoch 54/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8407 - loss: 0.5182 \n",
      "Epoch 55/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7525 - loss: 0.5522 \n",
      "Epoch 56/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7512 - loss: 0.5282 \n",
      "Epoch 57/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8046 - loss: 0.5127 \n",
      "Epoch 58/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8322 - loss: 0.5047 \n",
      "Epoch 59/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7048 - loss: 0.5522 \n",
      "Epoch 60/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8050 - loss: 0.5177 \n",
      "Epoch 61/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8339 - loss: 0.4793 \n",
      "Epoch 62/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8268 - loss: 0.4980 \n",
      "Epoch 63/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8280 - loss: 0.4880 \n",
      "Epoch 64/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8480 - loss: 0.4757 \n",
      "Epoch 65/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8642 - loss: 0.4689 \n",
      "Epoch 66/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8514 - loss: 0.4793 \n",
      "Epoch 67/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7857 - loss: 0.4988 \n",
      "Epoch 68/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7963 - loss: 0.4889 \n",
      "Epoch 69/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8157 - loss: 0.4636 \n",
      "Epoch 70/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8248 - loss: 0.4592 \n",
      "Epoch 71/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8083 - loss: 0.4799 \n",
      "Epoch 72/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8097 - loss: 0.4576 \n",
      "Epoch 73/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8338 - loss: 0.4304  \n",
      "Epoch 74/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8459 - loss: 0.4472 \n",
      "Epoch 75/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8972 - loss: 0.4385 \n",
      "Epoch 76/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8149 - loss: 0.4619 \n",
      "Epoch 77/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8561 - loss: 0.4591 \n",
      "Epoch 78/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8471 - loss: 0.4185 \n",
      "Epoch 79/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9184 - loss: 0.4007 \n",
      "Epoch 80/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8998 - loss: 0.4273 \n",
      "Epoch 81/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8461 - loss: 0.4315 \n",
      "Epoch 82/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8955 - loss: 0.3840 \n",
      "Epoch 83/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8860 - loss: 0.4160 \n",
      "Epoch 84/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8948 - loss: 0.4004 \n",
      "Epoch 85/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9281 - loss: 0.3749 \n",
      "Epoch 86/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8582 - loss: 0.4235 \n",
      "Epoch 87/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9108 - loss: 0.3862 \n",
      "Epoch 88/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9050 - loss: 0.3471 \n",
      "Epoch 89/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9372 - loss: 0.3725 \n",
      "Epoch 90/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8598 - loss: 0.4145 \n",
      "Epoch 91/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9418 - loss: 0.3408 \n",
      "Epoch 92/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9209 - loss: 0.3425 \n",
      "Epoch 93/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9526 - loss: 0.3229 \n",
      "Epoch 94/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8735 - loss: 0.3769 \n",
      "Epoch 95/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9328 - loss: 0.3379 \n",
      "Epoch 96/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9324 - loss: 0.3391 \n",
      "Epoch 97/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9438 - loss: 0.3641 \n",
      "Epoch 98/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9362 - loss: 0.3358 \n",
      "Epoch 99/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9265 - loss: 0.3298 \n",
      "Epoch 100/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9421 - loss: 0.3210 \n",
      "Epoch 101/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9746 - loss: 0.2837 \n",
      "Epoch 102/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9298 - loss: 0.3342 \n",
      "Epoch 103/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9766 - loss: 0.2887 \n",
      "Epoch 104/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9086 - loss: 0.2996\n",
      "Epoch 105/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9546 - loss: 0.2899 \n",
      "Epoch 106/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9380 - loss: 0.2947 \n",
      "Epoch 107/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9427 - loss: 0.2976 \n",
      "Epoch 108/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9558 - loss: 0.2732 \n",
      "Epoch 109/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9607 - loss: 0.2667 \n",
      "Epoch 110/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9676 - loss: 0.2652 \n",
      "Epoch 111/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9698 - loss: 0.2634 \n",
      "Epoch 112/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9892 - loss: 0.2513 \n",
      "Epoch 113/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9589 - loss: 0.2532 \n",
      "Epoch 114/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9512 - loss: 0.2889 \n",
      "Epoch 115/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9576 - loss: 0.2497 \n",
      "Epoch 116/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9738 - loss: 0.2497 \n",
      "Epoch 117/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9594 - loss: 0.2481 \n",
      "Epoch 118/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9804 - loss: 0.2460 \n",
      "Epoch 119/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9576 - loss: 0.2487 \n",
      "Epoch 120/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9635 - loss: 0.2492 \n",
      "Epoch 121/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9846 - loss: 0.2079 \n",
      "Epoch 122/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9341 - loss: 0.2644 \n",
      "Epoch 123/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9695 - loss: 0.2097 \n",
      "Epoch 124/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9769 - loss: 0.2121 \n",
      "Epoch 125/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9586 - loss: 0.2170 \n",
      "Epoch 126/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9610 - loss: 0.2058 \n",
      "Epoch 127/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9891 - loss: 0.2247 \n",
      "Epoch 128/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9624 - loss: 0.2028 \n",
      "Epoch 129/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9639 - loss: 0.2189 \n",
      "Epoch 130/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9779 - loss: 0.1826 \n",
      "Epoch 131/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9874 - loss: 0.1971 \n",
      "Epoch 132/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9540 - loss: 0.1997 \n",
      "Epoch 133/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9748 - loss: 0.1851 \n",
      "Epoch 134/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9874 - loss: 0.1593 \n",
      "Epoch 135/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9896 - loss: 0.1823 \n",
      "Epoch 136/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9904 - loss: 0.1719 \n",
      "Epoch 137/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9891 - loss: 0.1681 \n",
      "Epoch 138/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9657 - loss: 0.1859 \n",
      "Epoch 139/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9667 - loss: 0.1806 \n",
      "Epoch 140/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9696 - loss: 0.1633 \n",
      "Epoch 141/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9932 - loss: 0.1598 \n",
      "Epoch 142/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9816 - loss: 0.1741 \n",
      "Epoch 143/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9972 - loss: 0.1457 \n",
      "Epoch 144/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9932 - loss: 0.1804 \n",
      "Epoch 145/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9932 - loss: 0.1558 \n",
      "Epoch 146/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9932 - loss: 0.1589 \n",
      "Epoch 147/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9982 - loss: 0.1345 \n",
      "Epoch 148/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1403 \n",
      "Epoch 149/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9846 - loss: 0.1524 \n",
      "Epoch 150/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1678 \n",
      "Epoch 151/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9904 - loss: 0.1319 \n",
      "Epoch 152/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9982 - loss: 0.1381 \n",
      "Epoch 153/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9891 - loss: 0.1469 \n",
      "Epoch 154/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9947 - loss: 0.1358 \n",
      "Epoch 155/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9725 - loss: 0.1356 \n",
      "Epoch 156/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9947 - loss: 0.1235 \n",
      "Epoch 157/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1271 \n",
      "Epoch 158/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.1229 \n",
      "Epoch 159/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9725 - loss: 0.1508 \n",
      "Epoch 160/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1285 \n",
      "Epoch 161/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9861 - loss: 0.1149 \n",
      "Epoch 162/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1127 \n",
      "Epoch 163/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1165 \n",
      "Epoch 164/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9982 - loss: 0.0987 \n",
      "Epoch 165/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1090 \n",
      "Epoch 166/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1195 \n",
      "Epoch 167/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1115 \n",
      "Epoch 168/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0899 \n",
      "Epoch 169/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0961 \n",
      "Epoch 170/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.1090 \n",
      "Epoch 171/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0869 \n",
      "Epoch 172/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0800 \n",
      "Epoch 173/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0956 \n",
      "Epoch 174/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0914 \n",
      "Epoch 175/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0787 \n",
      "Epoch 176/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0990 \n",
      "Epoch 177/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0765 \n",
      "Epoch 178/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0920 \n",
      "Epoch 179/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0877 \n",
      "Epoch 180/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0875 \n",
      "Epoch 181/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0800 \n",
      "Epoch 182/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0793 \n",
      "Epoch 183/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0900 \n",
      "Epoch 184/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0715 \n",
      "Epoch 185/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0799 \n",
      "Epoch 186/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0658 \n",
      "Epoch 187/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0699 \n",
      "Epoch 188/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0812 \n",
      "Epoch 189/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0683 \n",
      "Epoch 190/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0595 \n",
      "Epoch 191/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0768 \n",
      "Epoch 192/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0567 \n",
      "Epoch 193/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0700 \n",
      "Epoch 194/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0656 \n",
      "Epoch 195/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0624 \n",
      "Epoch 196/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0626 \n",
      "Epoch 197/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0519 \n",
      "Epoch 198/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0607 \n",
      "Epoch 199/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0601 \n",
      "Epoch 200/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0663 \n",
      "Epoch 201/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0492 \n",
      "Epoch 202/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0572 \n",
      "Epoch 203/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0649 \n",
      "Epoch 204/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0672 \n",
      "Epoch 205/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0614 \n",
      "Epoch 206/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0544 \n",
      "Epoch 207/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0576 \n",
      "Epoch 208/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0477 \n",
      "Epoch 209/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0476 \n",
      "Epoch 210/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0522 \n",
      "Epoch 211/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0530 \n",
      "Epoch 212/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0416 \n",
      "Epoch 213/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0462 \n",
      "Epoch 214/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0485 \n",
      "Epoch 215/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0419 \n",
      "Epoch 216/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0430 \n",
      "Epoch 217/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0485 \n",
      "Epoch 218/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0473 \n",
      "Epoch 219/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0440 \n",
      "Epoch 220/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0447 \n",
      "Epoch 221/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0425 \n",
      "Epoch 222/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0479 \n",
      "Epoch 223/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0415 \n",
      "Epoch 224/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0392 \n",
      "Epoch 225/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0364 \n",
      "Epoch 226/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0444 \n",
      "Epoch 227/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0357 \n",
      "Epoch 228/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0360 \n",
      "Epoch 229/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0385 \n",
      "Epoch 230/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0411 \n",
      "Epoch 231/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0348 \n",
      "Epoch 232/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0386 \n",
      "Epoch 233/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0399 \n",
      "Epoch 234/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0336 \n",
      "Epoch 235/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0327 \n",
      "Epoch 236/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0303 \n",
      "Epoch 237/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0296 \n",
      "Epoch 238/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0364 \n",
      "Epoch 239/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0307 \n",
      "Epoch 240/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0256 \n",
      "Epoch 241/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0328 \n",
      "Epoch 242/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0291 \n",
      "Epoch 243/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0276 \n",
      "Epoch 244/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0313 \n",
      "Epoch 245/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0262 \n",
      "Epoch 246/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0257 \n",
      "Epoch 247/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0230 \n",
      "Epoch 248/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0269 \n",
      "Epoch 249/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0289 \n",
      "Epoch 250/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0257 \n",
      "Epoch 251/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0277 \n",
      "Epoch 252/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0351 \n",
      "Epoch 253/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0233 \n",
      "Epoch 254/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0240 \n",
      "Epoch 255/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0224 \n",
      "Epoch 256/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0233 \n",
      "Epoch 257/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0289 \n",
      "Epoch 258/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0220 \n",
      "Epoch 259/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0225 \n",
      "Epoch 260/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0227 \n",
      "Epoch 261/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0212 \n",
      "Epoch 262/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0236 \n",
      "Epoch 263/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0218 \n",
      "Epoch 264/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0184 \n",
      "Epoch 265/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0232 \n",
      "Epoch 266/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0190 \n",
      "Epoch 267/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0238 \n",
      "Epoch 268/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0196 \n",
      "Epoch 269/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0192 \n",
      "Epoch 270/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0160 \n",
      "Epoch 271/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0215 \n",
      "Epoch 272/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0222 \n",
      "Epoch 273/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0223 \n",
      "Epoch 274/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0188 \n",
      "Epoch 275/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0244 \n",
      "Epoch 276/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0180 \n",
      "Epoch 277/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0198 \n",
      "Epoch 278/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0189 \n",
      "Epoch 279/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0186 \n",
      "Epoch 280/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0172 \n",
      "Epoch 281/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0160 \n",
      "Epoch 282/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0195 \n",
      "Epoch 283/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0190 \n",
      "Epoch 284/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0147 \n",
      "Epoch 285/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0142 \n",
      "Epoch 286/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0142 \n",
      "Epoch 287/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0145 \n",
      "Epoch 288/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0146 \n",
      "Epoch 289/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0148 \n",
      "Epoch 290/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0179 \n",
      "Epoch 291/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0129 \n",
      "Epoch 292/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0139 \n",
      "Epoch 293/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0145 \n",
      "Epoch 294/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0151 \n",
      "Epoch 295/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0177 \n",
      "Epoch 296/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0121 \n",
      "Epoch 297/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0154 \n",
      "Epoch 298/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0135 \n",
      "Epoch 299/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0144\n",
      "Epoch 300/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0148 \n",
      "Epoch 301/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0152 \n",
      "Epoch 302/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0132 \n",
      "Epoch 303/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0117 \n",
      "Epoch 304/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0139 \n",
      "Epoch 305/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0107 \n",
      "Epoch 306/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0113 \n",
      "Epoch 307/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0115 \n",
      "Epoch 308/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0129 \n",
      "Epoch 309/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0144 \n",
      "Epoch 310/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0124 \n",
      "Epoch 311/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0120 \n",
      "Epoch 312/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0133 \n",
      "Epoch 313/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0119 \n",
      "Epoch 314/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0112 \n",
      "Epoch 315/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0111 \n",
      "Epoch 316/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0121 \n",
      "Epoch 317/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0122 \n",
      "Epoch 318/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0087\n",
      "Epoch 319/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0095 \n",
      "Epoch 320/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0097 \n",
      "Epoch 321/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0107 \n",
      "Epoch 322/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0097 \n",
      "Epoch 323/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0103 \n",
      "Epoch 324/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0106 \n",
      "Epoch 325/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0107 \n",
      "Epoch 326/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0094 \n",
      "Epoch 327/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0116 \n",
      "Epoch 328/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0093 \n",
      "Epoch 329/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0094 \n",
      "Epoch 330/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0100 \n",
      "Epoch 331/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0095 \n",
      "Epoch 332/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0107 \n",
      "Epoch 333/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0097 \n",
      "Epoch 334/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0075 \n",
      "Epoch 335/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0099 \n",
      "Epoch 336/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0113 \n",
      "Epoch 337/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0076 \n",
      "Epoch 338/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0087 \n",
      "Epoch 339/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0084 \n",
      "Epoch 340/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0084 \n",
      "Epoch 341/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0077 \n",
      "Epoch 342/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0074 \n",
      "Epoch 343/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0079 \n",
      "Epoch 344/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0083 \n",
      "Epoch 345/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0072 \n",
      "Epoch 346/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0090 \n",
      "Epoch 347/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0082 \n",
      "Epoch 348/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0074 \n",
      "Epoch 349/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0077 \n",
      "Epoch 350/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0072 \n",
      "Epoch 351/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0075 \n",
      "Epoch 352/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0077 \n",
      "Epoch 353/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0069 \n",
      "Epoch 354/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0063 \n",
      "Epoch 355/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0077 \n",
      "Epoch 356/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0079 \n",
      "Epoch 357/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0082 \n",
      "Epoch 358/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0073 \n",
      "Epoch 359/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0071 \n",
      "Epoch 360/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0083 \n",
      "Epoch 361/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0061 \n",
      "Epoch 362/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0074 \n",
      "Epoch 363/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0067 \n",
      "Epoch 364/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0073 \n",
      "Epoch 365/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0064 \n",
      "Epoch 366/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0061 \n",
      "Epoch 367/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0064 \n",
      "Epoch 368/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0058 \n",
      "Epoch 369/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0057 \n",
      "Epoch 370/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0049 \n",
      "Epoch 371/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0053 \n",
      "Epoch 372/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0065 \n",
      "Epoch 373/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0061 \n",
      "Epoch 374/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0054 \n",
      "Epoch 375/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0059 \n",
      "Epoch 376/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0069 \n",
      "Epoch 377/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0057 \n",
      "Epoch 378/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0068 \n",
      "Epoch 379/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0048 \n",
      "Epoch 380/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0058 \n",
      "Epoch 381/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0048 \n",
      "Epoch 382/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0055 \n",
      "Epoch 383/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0059 \n",
      "Epoch 384/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0052 \n",
      "Epoch 385/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0053 \n",
      "Epoch 386/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0051 \n",
      "Epoch 387/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0046 \n",
      "Epoch 388/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0042 \n",
      "Epoch 389/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0041 \n",
      "Epoch 390/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0054 \n",
      "Epoch 391/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0048 \n",
      "Epoch 392/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0041 \n",
      "Epoch 393/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0050 \n",
      "Epoch 394/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0039 \n",
      "Epoch 395/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0049 \n",
      "Epoch 396/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0040 \n",
      "Epoch 397/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0037 \n",
      "Epoch 398/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0045 \n",
      "Epoch 399/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0047 \n",
      "Epoch 400/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0055 \n",
      "Epoch 401/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0044 \n",
      "Epoch 402/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0042 \n",
      "Epoch 403/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0044 \n",
      "Epoch 404/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0048 \n",
      "Epoch 405/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0048 \n",
      "Epoch 406/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0046 \n",
      "Epoch 407/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0048 \n",
      "Epoch 408/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0044 \n",
      "Epoch 409/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0050 \n",
      "Epoch 410/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0041 \n",
      "Epoch 411/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0033 \n",
      "Epoch 412/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0046 \n",
      "Epoch 413/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0031 \n",
      "Epoch 414/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0042 \n",
      "Epoch 415/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0040 \n",
      "Epoch 416/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0043 \n",
      "Epoch 417/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0040 \n",
      "Epoch 418/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0034 \n",
      "Epoch 419/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0041 \n",
      "Epoch 420/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0038 \n",
      "Epoch 421/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0043 \n",
      "Epoch 422/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0033 \n",
      "Epoch 423/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0044 \n",
      "Epoch 424/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0033 \n",
      "Epoch 425/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0033 \n",
      "Epoch 426/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0026 \n",
      "Epoch 427/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0033 \n",
      "Epoch 428/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0037 \n",
      "Epoch 429/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0033 \n",
      "Epoch 430/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0037 \n",
      "Epoch 431/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0030 \n",
      "Epoch 432/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0036 \n",
      "Epoch 433/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0034 \n",
      "Epoch 434/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0029 \n",
      "Epoch 435/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0029 \n",
      "Epoch 436/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0029 \n",
      "Epoch 437/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0028 \n",
      "Epoch 438/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0032 \n",
      "Epoch 439/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0034  \n",
      "Epoch 440/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0028 \n",
      "Epoch 441/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0034 \n",
      "Epoch 442/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0031 \n",
      "Epoch 443/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0027 \n",
      "Epoch 444/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0027 \n",
      "Epoch 445/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0027 \n",
      "Epoch 446/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0030 \n",
      "Epoch 447/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0024 \n",
      "Epoch 448/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0023 \n",
      "Epoch 449/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0034 \n",
      "Epoch 450/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0028 \n",
      "Epoch 451/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0028  \n",
      "Epoch 452/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0026 \n",
      "Epoch 453/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0028 \n",
      "Epoch 454/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0028 \n",
      "Epoch 455/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0032 \n",
      "Epoch 456/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0023     \n",
      "Epoch 457/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0027 \n",
      "Epoch 458/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0024 \n",
      "Epoch 459/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0022     \n",
      "Epoch 460/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0027 \n",
      "Epoch 461/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0029 \n",
      "Epoch 462/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0028 \n",
      "Epoch 463/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0028 \n",
      "Epoch 464/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0026 \n",
      "Epoch 465/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0024 \n",
      "Epoch 466/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0028 \n",
      "Epoch 467/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0021 \n",
      "Epoch 468/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0024 \n",
      "Epoch 469/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0024 \n",
      "Epoch 470/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0025 \n",
      "Epoch 471/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0019 \n",
      "Epoch 472/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0018 \n",
      "Epoch 473/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0022 \n",
      "Epoch 474/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0022 \n",
      "Epoch 475/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0024 \n",
      "Epoch 476/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0021 \n",
      "Epoch 477/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0019 \n",
      "Epoch 478/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0016 \n",
      "Epoch 479/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0022 \n",
      "Epoch 480/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0019 \n",
      "Epoch 481/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0023 \n",
      "Epoch 482/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 483/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0018 \n",
      "Epoch 484/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0025 \n",
      "Epoch 485/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0024 \n",
      "Epoch 486/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0020 \n",
      "Epoch 487/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0020 \n",
      "Epoch 488/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0020 \n",
      "Epoch 489/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0019 \n",
      "Epoch 490/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0016     \n",
      "Epoch 491/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0021 \n",
      "Epoch 492/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0019 \n",
      "Epoch 493/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0018 \n",
      "Epoch 494/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0022 \n",
      "Epoch 495/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0018 \n",
      "Epoch 496/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0014     \n",
      "Epoch 497/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0016 \n",
      "Epoch 498/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0016 \n",
      "Epoch 499/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0021 \n",
      "Epoch 500/500\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0017 \n",
      "{'accuracy': [0.4699999988079071, 0.5400000214576721, 0.5600000023841858, 0.6100000143051147, 0.6200000047683716, 0.5799999833106995, 0.6000000238418579, 0.6399999856948853, 0.6200000047683716, 0.6299999952316284, 0.6299999952316284, 0.6299999952316284, 0.6299999952316284, 0.6200000047683716, 0.6399999856948853, 0.6200000047683716, 0.6399999856948853, 0.6700000166893005, 0.6499999761581421, 0.6499999761581421, 0.6800000071525574, 0.6499999761581421, 0.6399999856948853, 0.6600000262260437, 0.6899999976158142, 0.6899999976158142, 0.699999988079071, 0.6800000071525574, 0.6800000071525574, 0.6600000262260437, 0.699999988079071, 0.6800000071525574, 0.7099999785423279, 0.7400000095367432, 0.7300000190734863, 0.7200000286102295, 0.7300000190734863, 0.7300000190734863, 0.7200000286102295, 0.7400000095367432, 0.7699999809265137, 0.7599999904632568, 0.7099999785423279, 0.7699999809265137, 0.7699999809265137, 0.75, 0.7699999809265137, 0.7400000095367432, 0.7300000190734863, 0.75, 0.7599999904632568, 0.7599999904632568, 0.7900000214576721, 0.7900000214576721, 0.7799999713897705, 0.7900000214576721, 0.7900000214576721, 0.7900000214576721, 0.7900000214576721, 0.7900000214576721, 0.7900000214576721, 0.800000011920929, 0.800000011920929, 0.8100000023841858, 0.8100000023841858, 0.8299999833106995, 0.8100000023841858, 0.800000011920929, 0.800000011920929, 0.8199999928474426, 0.8199999928474426, 0.8299999833106995, 0.8299999833106995, 0.8399999737739563, 0.8600000143051147, 0.8600000143051147, 0.8399999737739563, 0.8500000238418579, 0.8799999952316284, 0.8899999856948853, 0.8799999952316284, 0.8899999856948853, 0.8899999856948853, 0.8899999856948853, 0.8999999761581421, 0.8899999856948853, 0.8999999761581421, 0.8500000238418579, 0.9100000262260437, 0.8799999952316284, 0.9100000262260437, 0.9200000166893005, 0.9100000262260437, 0.9100000262260437, 0.9300000071525574, 0.9300000071525574, 0.9399999976158142, 0.9399999976158142, 0.9300000071525574, 0.9399999976158142, 0.9300000071525574, 0.949999988079071, 0.949999988079071, 0.9300000071525574, 0.9599999785423279, 0.9100000262260437, 0.9399999976158142, 0.9300000071525574, 0.949999988079071, 0.949999988079071, 0.949999988079071, 0.9700000286102295, 0.9700000286102295, 0.9700000286102295, 0.9300000071525574, 0.9700000286102295, 0.9599999785423279, 0.9700000286102295, 0.9700000286102295, 0.949999988079071, 0.9700000286102295, 0.9700000286102295, 0.9700000286102295, 0.9700000286102295, 0.9800000190734863, 0.949999988079071, 0.9900000095367432, 0.9700000286102295, 0.9800000190734863, 0.9700000286102295, 0.9800000190734863, 0.9800000190734863, 0.9800000190734863, 0.9800000190734863, 0.9800000190734863, 0.9800000190734863, 0.9900000095367432, 0.9800000190734863, 0.9700000286102295, 0.9800000190734863, 0.9800000190734863, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 1.0, 0.9800000190734863, 0.9900000095367432, 0.9800000190734863, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 1.0, 1.0, 0.9900000095367432, 1.0, 0.9900000095367432, 1.0, 1.0, 0.9900000095367432, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'loss': [0.6983168721199036, 0.6917098760604858, 0.6865052580833435, 0.6844154596328735, 0.6810507774353027, 0.6784889101982117, 0.6778091192245483, 0.6746291518211365, 0.6720815896987915, 0.6701774597167969, 0.6681367754936218, 0.6661666631698608, 0.6647696495056152, 0.661681056022644, 0.6593025326728821, 0.658526599407196, 0.6543121933937073, 0.6538113355636597, 0.651740312576294, 0.6493580341339111, 0.6462979912757874, 0.6441152691841125, 0.6409158110618591, 0.6381454467773438, 0.6361108422279358, 0.6343636512756348, 0.6311665177345276, 0.6286273002624512, 0.6282371282577515, 0.6252131462097168, 0.6234103441238403, 0.6221104860305786, 0.6162344217300415, 0.6123215556144714, 0.6082603931427002, 0.6046441793441772, 0.601503312587738, 0.5986100435256958, 0.5954734683036804, 0.5918695330619812, 0.5875620245933533, 0.5837097764015198, 0.5832313299179077, 0.5755180716514587, 0.5756815075874329, 0.5718512535095215, 0.566288411617279, 0.5648135542869568, 0.5600413680076599, 0.559644341468811, 0.5587224364280701, 0.5493121147155762, 0.5441064238548279, 0.5374495387077332, 0.5351684093475342, 0.5345396399497986, 0.5272603034973145, 0.5230672359466553, 0.5203476548194885, 0.5153090953826904, 0.5099174380302429, 0.509317934513092, 0.5009397268295288, 0.496805340051651, 0.4917433261871338, 0.4875026047229767, 0.49100300669670105, 0.48024651408195496, 0.4783801734447479, 0.46665632724761963, 0.468251496553421, 0.46517303586006165, 0.4611983001232147, 0.45558276772499084, 0.448142409324646, 0.4428572356700897, 0.43723204731941223, 0.43213731050491333, 0.42468392848968506, 0.420326828956604, 0.41546496748924255, 0.4118403196334839, 0.40484142303466797, 0.40099430084228516, 0.39742153882980347, 0.3921447694301605, 0.3863523006439209, 0.3848216235637665, 0.37598174810409546, 0.380594402551651, 0.3767753839492798, 0.3616718351840973, 0.3604849874973297, 0.3527361750602722, 0.3471570611000061, 0.3450968861579895, 0.3378192186355591, 0.334408164024353, 0.33060595393180847, 0.3240424692630768, 0.3211399018764496, 0.3147970139980316, 0.3072451651096344, 0.30831488966941833, 0.3028354346752167, 0.3092459738254547, 0.2994278371334076, 0.2947632670402527, 0.28422868251800537, 0.28023606538772583, 0.2729305624961853, 0.27120867371559143, 0.2659267485141754, 0.2626212239265442, 0.26711341738700867, 0.2624761462211609, 0.2485206425189972, 0.2505739629268646, 0.23962870240211487, 0.24137382209300995, 0.23249287903308868, 0.23220036923885345, 0.23297171294689178, 0.2241595983505249, 0.21959352493286133, 0.22238293290138245, 0.21929500997066498, 0.20825625956058502, 0.20357654988765717, 0.20108118653297424, 0.19825367629528046, 0.1944512128829956, 0.19055122137069702, 0.18745987117290497, 0.18612617254257202, 0.17705081403255463, 0.18323440849781036, 0.17431432008743286, 0.17569288611412048, 0.1695442646741867, 0.16540178656578064, 0.16609469056129456, 0.16224370896816254, 0.1573287397623062, 0.15472935140132904, 0.15207701921463013, 0.15528099238872528, 0.15638281404972076, 0.1514526605606079, 0.14617197215557098, 0.14208486676216125, 0.13778109848499298, 0.13413377106189728, 0.13167984783649445, 0.13266456127166748, 0.1289459764957428, 0.12900735437870026, 0.11989431083202362, 0.1232052892446518, 0.1195552796125412, 0.11952941864728928, 0.11280254274606705, 0.11213766783475876, 0.1120486631989479, 0.11380172520875931, 0.11525397002696991, 0.10662573575973511, 0.10151689499616623, 0.10289470851421356, 0.09951939433813095, 0.09730788320302963, 0.09485635161399841, 0.0942467525601387, 0.09500637650489807, 0.09383419156074524, 0.08939951658248901, 0.0881800577044487, 0.08784358203411102, 0.08510890603065491, 0.08318649232387543, 0.08369743078947067, 0.08438894152641296, 0.0811273604631424, 0.0784507691860199, 0.07761280983686447, 0.07504360377788544, 0.07606903463602066, 0.07242486625909805, 0.07223016768693924, 0.07078235596418381, 0.06906856596469879, 0.06750603765249252, 0.06810539960861206, 0.06628604233264923, 0.06476237624883652, 0.06475110352039337, 0.06289476901292801, 0.06262736022472382, 0.06069377809762955, 0.06056195870041847, 0.06004006043076515, 0.05764012783765793, 0.057089563459157944, 0.05967147648334503, 0.05694062262773514, 0.054070793092250824, 0.05288740247488022, 0.05374489724636078, 0.05250502750277519, 0.051809921860694885, 0.052723027765750885, 0.04913298040628433, 0.048261791467666626, 0.04725326970219612, 0.04687296971678734, 0.04635125771164894, 0.044490426778793335, 0.04433669522404671, 0.043577324599027634, 0.04243369400501251, 0.04220470413565636, 0.041978854686021805, 0.040452826768159866, 0.04099804908037186, 0.040350448340177536, 0.04018218070268631, 0.03821396455168724, 0.03750170022249222, 0.0368981808423996, 0.036183301359415054, 0.03628278523683548, 0.03470033034682274, 0.034701600670814514, 0.034933775663375854, 0.03344975411891937, 0.03255588188767433, 0.03240726515650749, 0.03304797783493996, 0.032078638672828674, 0.030617842450737953, 0.030360043048858643, 0.029897799715399742, 0.02986549213528633, 0.02918236330151558, 0.02845894545316696, 0.027759291231632233, 0.02794262394309044, 0.027219133451581, 0.027322910726070404, 0.027721036225557327, 0.026293104514479637, 0.02620537579059601, 0.02545386739075184, 0.02490011230111122, 0.024725472554564476, 0.023971829563379288, 0.024930184707045555, 0.023623371496796608, 0.023481523618102074, 0.022655488923192024, 0.02279938943684101, 0.022382155060768127, 0.022966919466853142, 0.021653397008776665, 0.02202191762626171, 0.021499767899513245, 0.02082182839512825, 0.02080913633108139, 0.020656030625104904, 0.02016734518110752, 0.0200723335146904, 0.02008134126663208, 0.019269786775112152, 0.019132159650325775, 0.01900186948478222, 0.01869089901447296, 0.018762951716780663, 0.01793527789413929, 0.017712311819195747, 0.017784249037504196, 0.018011868000030518, 0.017288612201809883, 0.017119990661740303, 0.016948197036981583, 0.016872836276888847, 0.016300363466143608, 0.016433777287602425, 0.015807010233402252, 0.01622145250439644, 0.01575206033885479, 0.015314326621592045, 0.015123150311410427, 0.014875936321914196, 0.014894562773406506, 0.014466299675405025, 0.014479260891675949, 0.014401018619537354, 0.014062992297112942, 0.01403748244047165, 0.013831624761223793, 0.013426925987005234, 0.013566241599619389, 0.01329727377742529, 0.01316127274185419, 0.01298090536147356, 0.012694417499005795, 0.012947402894496918, 0.012447591871023178, 0.01253575924783945, 0.012220707722008228, 0.012111969292163849, 0.012348074465990067, 0.012123201973736286, 0.011827143840491772, 0.011442247778177261, 0.011558047495782375, 0.011153478175401688, 0.010967488400638103, 0.010922068729996681, 0.010722347535192966, 0.010675923898816109, 0.010519696399569511, 0.010497938841581345, 0.01030381303280592, 0.010230336338281631, 0.010137886740267277, 0.010081515647470951, 0.009843811392784119, 0.00978025421500206, 0.009871662594377995, 0.009558848105370998, 0.009555903263390064, 0.00948859192430973, 0.009445058181881905, 0.009379980154335499, 0.00903967022895813, 0.008891609497368336, 0.008839494548738003, 0.008893192745745182, 0.008774484507739544, 0.008923168294131756, 0.008364886976778507, 0.008456425741314888, 0.008746684528887272, 0.00830729678273201, 0.00824076309800148, 0.008088470436632633, 0.008042516186833382, 0.00798942893743515, 0.007736368104815483, 0.007854714058339596, 0.007552657276391983, 0.007397017907351255, 0.007468119263648987, 0.007369528990238905, 0.007460372522473335, 0.007041145581752062, 0.007164106238633394, 0.007111316546797752, 0.007350920233875513, 0.00688078161329031, 0.006833868101239204, 0.006780301220715046, 0.006659203674644232, 0.0065120067447423935, 0.006508739199489355, 0.006487210281193256, 0.006358979735523462, 0.0062462687492370605, 0.006306174211204052, 0.00609469972550869, 0.006173348519951105, 0.00603498937562108, 0.006036675535142422, 0.00597797567024827, 0.005982755217701197, 0.005817209370434284, 0.005872100591659546, 0.005700665060430765, 0.005889090709388256, 0.005691994912922382, 0.0055565913207829, 0.0055287922732532024, 0.005453774239867926, 0.005402678158134222, 0.005353659391403198, 0.005200117360800505, 0.0052682142704725266, 0.005342376418411732, 0.005400457885116339, 0.005008688196539879, 0.005184365902096033, 0.004891672637313604, 0.004827613942325115, 0.00485707214102149, 0.004883080720901489, 0.004704361781477928, 0.004662634339183569, 0.004540273919701576, 0.004586678929626942, 0.004681180231273174, 0.004576032515615225, 0.004372268449515104, 0.004389486741274595, 0.004282710142433643, 0.004295864142477512, 0.00423006433993578, 0.004153014160692692, 0.004133809357881546, 0.004166914150118828, 0.004061111714690924, 0.003975404892116785, 0.0039059442933648825, 0.003960456699132919, 0.0040025473572313786, 0.00390641950070858, 0.003858578158542514, 0.003761411877349019, 0.003757267026230693, 0.003718923544511199, 0.003634034190326929, 0.003632181789726019, 0.0035757245495915413, 0.0035341507755219936, 0.0034809429198503494, 0.003486284986138344, 0.003453814657405019, 0.003409709082916379, 0.0034513601567596197, 0.0034303178545087576, 0.0033076852560043335, 0.0034388252533972263, 0.003275273134931922, 0.003261461155489087, 0.003246115520596504, 0.0032721736934036016, 0.0031522817444056273, 0.003125781426206231, 0.003125161165371537, 0.003016981529071927, 0.002996781375259161, 0.0029955641366541386, 0.0029677909333258867, 0.002910060342401266, 0.0029563531279563904, 0.0028135136235505342, 0.0029081464745104313, 0.002903772285208106, 0.002845096867531538, 0.0028040611650794744, 0.0028282194398343563, 0.002707791281864047, 0.002687051659449935, 0.002662478946149349, 0.002700027311220765, 0.0026885741390287876, 0.0026309408713132143, 0.0025786568876355886, 0.0025578748900443316, 0.0025043347850441933, 0.0025161749217659235, 0.0025060283951461315, 0.0025076703168451786, 0.0024589053355157375, 0.0024219013284891844, 0.002396917436271906, 0.0023836917243897915, 0.002315957099199295, 0.002408950589597225, 0.0024948313366621733, 0.0023042631801217794, 0.0023608359042555094, 0.002345473738387227, 0.0022317077964544296, 0.0021995948627591133, 0.0021854841616004705, 0.0021826389711350203, 0.002131664426997304, 0.0021087373606860638, 0.002088014967739582, 0.0020776805467903614, 0.0020429829601198435, 0.002060634084045887, 0.0019937995821237564, 0.0020122709684073925, 0.001973387785255909, 0.0019491292769089341, 0.0019359237048774958, 0.0019427624065428972, 0.0018993630073964596, 0.0018849330954253674, 0.0019085954409092665, 0.0018520927987992764, 0.001896351226605475, 0.0018270584987476468, 0.0018144366331398487, 0.001818441553041339, 0.0018030275823548436, 0.0017878510989248753, 0.001760933082550764]}\n"
     ]
    }
   ],
   "source": [
    "# mm=neural_net()\n",
    "# history=mm.fit(XX,dummy_y,epochs=500)\n",
    "XX = np.random.rand(100, 10)  # 100 samples, 10 features\n",
    "dummy_y = np.random.randint(2, size=(100, 1))  # Binary classification labels\n",
    "\n",
    "# Define the neural network model\n",
    "mm = Sequential()\n",
    "mm.add(Dense(64, input_dim=10, activation='relu'))  # Input layer\n",
    "mm.add(Dense(32, activation='relu'))  # Hidden layer\n",
    "mm.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "mm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = mm.fit(XX, dummy_y, epochs=500, batch_size=10)\n",
    "\n",
    "# Access training history\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ANN Model Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
